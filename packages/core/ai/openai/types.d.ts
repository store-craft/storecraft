export * from './index.js';

export type config = {
  model?: ('o1-mini' | 'gpt-4o' | 'gpt-4' | 'gpt-4o-mini' | 'gpt-4-turbo') | ({} & string),
  endpoint?: string,
  api_version?: string;
  api_key: string
}

export type roles = 'user' | 'tool' | 'assistant' | 'system';

/** Developer-provided instructions that the model should follow, regardless of messages sent by the user. With o1 models and newer, use developer messages for this purpose instead. */
export type developer_message = {
  content: string | string[];
  role: 'developer';
}

/** Developer-provided instructions that the model should follow, regardless of messages sent by the user. With o1 models and newer, use developer messages for this purpose instead. */
export type system_message = {
  content: string;
  role: 'system';
  /** An optional name for the participant. Provides the model information to differentiate between participants of the same role. */
  name?: string;
}

/** Text content part */
export type user_message_content_text_part = {
  type: 'text',
  text: string
}

/** Image content part */
export type user_message_content_image_part = {
  type: 'image_url',
  image_url: {
    /** Either a URL of the image or the base64 encoded image data. */
    url: string;
    /** Specifies the detail level of the image. Learn more in the Vision guide. */
    detail?: 'auto' | 'low' | 'high';
  }
}

/** Audio content part */
export type user_message_content_audio_part = {
  type: 'input_audio',
  input_audio: {
    /** Base64 encoded audio data. */
    data: string;
    /** Specifies the detail level of the image. Learn more in the Vision guide. */
    format: 'wav' | 'mp3';
  }
}


/** Messages sent by an end user, containing prompts or additional context information. */
export type user_message = {
  content: string | user_message_content_text_part | 
      user_message_content_image_part | 
      user_message_content_audio_part;
  role: 'user';
  /** An optional name for the participant. Provides the model information to differentiate between participants of the same role. */
  name?: string;
}

/** Messages sent by the model in response to user messages. */
export type assistant_message = {
  /** An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal */
  content: string | (
    {
      type: string;
      /** The text content. */
      text: string;
    } | {
      /** The type of the content part. */
      type: string;
      /** The refusal message generated by the model */
      refusal: string;
    }
  )[];
  refusal?: string;
  role: 'assistant';
  /** An optional name for the participant. Provides the model information to differentiate between participants of the same role. */
  name?: string;
  tool_calls?: {
    /** The ID of the tool call. */
    id: string;
    type: 'function';
    function: {
      /** The name of the function to call. */
      name: string;
      /** The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function. */
      arguments: string;
    }
  }[];
  /** Data about a previous audio response from the model. Learn more. */
  audio?: {
    /** Unique identifier for a previous audio response from the model. */
    id: string;
  }
}

export type tool_message = {
  content: string | string[];
  role: 'tool';
  /** Tool call that this message is responding to. **/
  tool_call_id: string;
}

export type chat_message = developer_message | system_message | user_message | assistant_message | tool_message;

export type chat_completion_input = {
  /** ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.*/
  model: string;
  /** A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, images, and audio. */
  messages: chat_message[];

  /** An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. */
  max_completion_tokens?: number;

  /** How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.*/
  n?: number;

  /** A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.*/
  tools?: {
    /** The type of the tool. Currently, only function is supported. */
    type: 'function' | string;
    function: {
      /** The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64. */
      name?: string;
      /** The parameters the functions accepts, described as a JSON Schema object. See the [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/reference/object) for documentation about the format. */
      parameters?: object;
      /** A description of what the function does, used by the model to choose when and how to call the function. */
      description: string;
    }
  }[];

  /** Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {"type": "function", "function": {"name": "my_function"}} forces the model to call that tool. `none` is the default when no tools are present. `auto` is the default if tools are present. */
  tool_choice?: 'none' | 'auto' | {
    /** The type of the tool. Currently, only function is supported. */
    type: string;
    function: {
      /** The name of the function to call. */
      name: string;
    }

  };

  /**
   * @description An object specifying the format that the model must output.
   * Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured Outputs 
   * which ensures the model will match your supplied JSON schema. Learn more in the 
   * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
   * 
   * Setting to `{ "type": "json_object" }` enables JSON mode, which ensures the message 
   * the model generates is valid JSON.
   * 
   * Important: when using JSON mode, you must also instruct the model to produce JSON yourself 
   * via a system or user message. Without this, the model may generate an unending stream of 
   * whitespace until the generation reaches the token limit, resulting in a long-running 
   * and seemingly "stuck" request. Also note that the message content may be partially 
   * cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` 
   * or the conversation exceeded the max context length.
   */
  response_format?: {
    /** The type of response format being defined: `text` */
    type: 'text';
  } | {
    /** The type of response format being defined: `json_object` */
    type: 'json_object';
  } | {
    /** The type of response format being defined: `json_schema` */
    type: 'json_schema';
    /** A description of what the response format is for, used by the model to determine how to respond in the format. */
    description?: string;
    /** The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64. */
    name: string;
    /** The schema for the response format, described as a JSON Schema object. */
    schema: object;
    /** Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true. To learn more, read the [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).*/
    strict?: boolean;
  }

  /** Up to 4 sequences where the API will stop generating further tokens. */
  stop?: string | string[] | null;

  /** If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. [Example Python Code](https://cookbook.openai.com/examples/how_to_stream_completions) */
  stream?: boolean;

  /** What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both. */
  temperature?: number;

  /** An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So `0.1` means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.
   * @default 1
   */
  top_p?: number;
}


export type chat_completion_result = {
  /** A unique identifier for the chat completion. */
  id: string;
  /** The Unix timestamp (in seconds) of when the chat completion was created.   */
  created: number;
  /** The model used for the chat completion. */
  model: string;
  /** A list of chat completion choices. Can be more than one if n is greater than 1. */
  choices: {
    /** The index of the choice in the list of choices. */
    index: number;
    /** A chat completion message generated by the model. */
    message: assistant_message,

    /** The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call (deprecated) if the model called a function. */
    finish_reason: 'stop' | 'length' | 'content_filter' | 'tool_calls';

    /** Log probability information for the choice. */
    logprobs?: object;
  }[];

  /** The object type, which is always `chat.completion` */
  object: 'chat.completion';

  /** The service tier used for processing the request. */
  service_tier?: string;

  /** This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism. */
  system_fingerprint: string;

  /** Usage statistics for the completion request. */
  usage: {
    /** Number of tokens in the generated completion */
    completion_tokens: number;
    /** Number of tokens in the prompt. */
    prompt_tokens: number;
    /** Total number of tokens used in the request (prompt + completion). */
    total_tokens: number;
    /** Breakdown of tokens used in a completion. */
    completion_tokens_details: {
      /** When using Predicted Outputs, the number of tokens in the prediction that appeared in the completion. */
      accepted_prediction_tokens: number;
      /** Audio input tokens generated by the model. */
      audio_tokens: number;
      /** Tokens generated by the model for reasoning. */
      reasoning_tokens: number;
      /** When using Predicted Outputs, the number of tokens in the prediction that did not appear in the completion. However, like reasoning tokens, these tokens are still counted in the total completion tokens for purposes of billing, output, and context window limits. */
      rejected_prediction_tokens: number;
    }
    /** Breakdown of tokens used in the prompt. */
    prompt_tokens_details: {
      /** Audio input tokens present in the prompt. */
      audio_tokens: number;
      /** Cached tokens present in the prompt. */
      cached_tokens: number;
    }
  }
}



export type chat_completion_chunk_result = {
  /** A unique identifier for the chat completion. */
  id: string;
  /** The Unix timestamp (in seconds) of when the chat completion was created.   */
  created: number;
  /** The model used for the chat completion. */
  model: string;

  /** A list of chat completion choices. Can be more than one if n is greater than 1. */
  choices: {
    /** The index of the choice in the list of choices. */
    index: number;
    /** A chat completion delta generated by streamed model responses. */
    delta: {
      /** The contents of the chunk message. */
      content: string | null;

      /** The refusal message generated by the model. */
      refusal: string | null;
      
      /** The role of the author of this message. */
      role: string;
      /** The tool calls generated by the model, such as function calls. */
      tool_calls: {
        index: number;
        /** The ID of the tool call. */
        id: string;
        /** The type of the tool. Currently, only function is supported */
        type: 'function' | (string & {});
        /** The function that the model called. */
        function: {
          /** The name of the function to call. */
          name: string;
          /** The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.*/
          arguments: string;
        }
      }[]

    },
    /** The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call (deprecated) if the model called a function. */
    finish_reason: 'stop'| 'length' | 'content_filter' | 'tool_calls',
    /** Log probability information for the choice. */
    logprobs: object;

  }[];

  /** The object type, which is always `chat.completion.chunk` */
  object: 'chat.completion.chunk';

  /** The service tier used for processing the request. */
  service_tier?: string;

  /** This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism. */
  system_fingerprint: string;

  /** An optional field that will only be present when you set stream_options: {"include_usage": true} in your request. When present, it contains a null value except for the last chunk which contains the token usage statistics for the entire request. */
  usage?: {
    /** Number of tokens in the generated completion. */
    completion_tokens: number;
    /** Number of tokens in the prompt. */
    prompt_tokens: number;
    /** Total number of tokens used in the request (prompt + completion). */
    total_tokens: number;
  }
}

